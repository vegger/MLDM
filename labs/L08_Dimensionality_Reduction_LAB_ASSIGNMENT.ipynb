{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegger/MLDM/blob/main/labs/L08_Dimensionality_Reduction_LAB_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openml"
      ],
      "metadata": {
        "id": "mHsOJeSvpzZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEnOZvlls6_o"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "RANDOM_SEED = 0xdeadbeef"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Warmup - Breast Cancer Classification (3 Points)\n",
        "\n",
        "In this warm-up task, we you will apply what you have learned so far about classification to a dataset of Breast Cancer samples. The dataset has 30 features and the goal is to predict whether a particular example is malignant or benign. In further tasks, we will apply dimensionality reduction techniques to this dataset. The goal of this first task is to establish a baseline that we can compare against."
      ],
      "metadata": {
        "id": "LXrtf5gUsMrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we load the data and print out the description of the dataset. You do not have to read it in detail but we suggest you have a quick look at what kind of measurements were taken."
      ],
      "metadata": {
        "id": "NAkbuWcpt9-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer(as_frame=True)\n",
        "X_raw = data.data\n",
        "y = data.target\n",
        "y.replace(0, 'Malignant', inplace=True)\n",
        "y.replace(1, 'Benign', inplace=True)\n",
        "\n",
        "print(data.DESCR)"
      ],
      "metadata": {
        "id": "hPJdhacmveTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we split the data in a train and test set. We select $80\\%$ of the data for training and the remaining to test. We also use stratification to make sure that the proportion of malignant and benign samples is the same in both the train and test set."
      ],
      "metadata": {
        "id": "QX8swkgFrfxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "  X_raw, y,\n",
        "  train_size=.8,  # use 80% for training\n",
        "  shuffle=True,   # randomize the split\n",
        "  random_state=RANDOM_SEED,  # set the random seed, so we always  get the same split \n",
        "  stratify=y,  # stratify, to make sure class proportions are preserved\n",
        ")"
      ],
      "metadata": {
        "id": "YwrquUbTukFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizing the data\n",
        "\n",
        "###  Task 1a. Apply standard (mean, standard deviation) normalization to both the training and the test data. \n",
        "\n",
        "**Remember:** we only compute the the mean and standard deviation on the training set!"
      ],
      "metadata": {
        "id": "X1vFxTqgvsKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = x_train.mean()\n",
        "std = x_train.std()\n",
        "\n",
        "x_train_norm = ...\n",
        "x_test_norm = ..."
      ],
      "metadata": {
        "id": "8uRZtc6oz5Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the performance of a Linear SVM classifier on this data?\n",
        "\n",
        "We are now ready to train our baseline classifier on this data. Since we will compare different classifiers to each other, it is useful to have a single score that we can compare.\n",
        "\n",
        "In the next cell, we provide a helper function to compute the F1 score for 'Malignant' predictions. We will use this function to compare classifiers."
      ],
      "metadata": {
        "id": "4orEjhEtxKjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_malignant(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
        "  \"\"\"\n",
        "  Helper function to compute the F1 score for the 'Malignant' class\n",
        "\n",
        "  y_true: a pd.Series object containing the ground truth labes \n",
        "          ('Benign' or 'Malignant'), you will mainly pass 'y_test' here\n",
        "  y_pred: a pd.Series object containing predicted labels, for example the \n",
        "          output of the '.predict' method of a classifier\n",
        "  \"\"\"\n",
        "  return f1_score(\n",
        "      y_true=y_true,\n",
        "      y_pred=y_pred,\n",
        "      pos_label=\"Malignant\",\n",
        "      average=\"binary\",\n",
        "  )"
      ],
      "metadata": {
        "id": "zJvx3NNIxTUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK1b. In the next cell, your task is to train linear SVM classifiers. We set up the loops to search for good values of the hyperparameters `C` and `class_weight` for you.\n",
        "\n",
        "Find the combination that gives the best (i.e. highest) value for `f1_malignant`."
      ],
      "metadata": {
        "id": "kYieT2RK2KXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "for c in [.1, 1., 10.]:\n",
        "  for class_weight in [None, \"balanced\"]:\n",
        "    svm = LinearSVC(C=c, class_weight=class_weight)\n",
        "\n",
        "    # fit svm\n",
        "    # compute f1_malignant\n",
        "    # find the best combination of C and class_weight\n"
      ],
      "metadata": {
        "id": "U7KHYOUyIIuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Enter in **Moodle** whether you solved this task."
      ],
      "metadata": {
        "id": "OV5eiCrXcJJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: PCA (5 Points)\n",
        "\n",
        "We will now apply Principal Component Analysis (PCA) to our data. At its core, PCA will just map our data into a new coordinate systems in such a way that the variation in the data can be explained with fewer dimensions.\n",
        "\n",
        "Note that in the original data space, the ordering of dimensions is arbitrary. For example, if you measure height and weight of people, there is no reason to prefer the height as first dimension and weight as second, over weight as first and height as second. \n",
        "\n",
        "In the new coordinate system that is computed by PCA, the dimensions are ordered by the amount of data variability they \"explain\". That means that the first dimension (the first \"Principal Component\") will explain more variability than the second, the second more than the third etc.\n",
        "\n",
        "This is the reason we can use PCA for dimensionality reduction. We only have to consider the subset of the first $k$ dimensions to get a good representation of the data.\n"
      ],
      "metadata": {
        "id": "nCKqo8mJ31g0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the original data\n",
        "\n",
        "In the next cells we print the feature names of our dataset and then choose 2 arbitrary dimensions to plot them against each other.\n",
        "\n",
        "You can change the values of the variables `feature1` and `feature2` to plot a different pair of features (dimensions)."
      ],
      "metadata": {
        "id": "sG2wLl2kMELG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = x_train_norm.columns\n",
        "print(feature_names)"
      ],
      "metadata": {
        "id": "qRuTPMZk5525"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature1 = \"mean radius\"\n",
        "feature2 = \"mean concavity\"\n",
        "\n",
        "plt.scatter(x_train_norm[feature1], x_train_norm[feature2], c=y_train.map({'Benign': 0, 'Malignant': 1}), s=50)\n",
        "plt.xlabel(feature1)\n",
        "plt.ylabel(feature2)"
      ],
      "metadata": {
        "id": "V7X6Ma-j7Iwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2a. How many Principal Components are needed to get a good representation of our data?\n",
        "\n",
        "In the next cell, we use the `PCA` implementation provided by `sklearn.decomposition`. As usual, this class provides a `.fit` method to learn the PCA coordinate transformation, and a `.transform` method to apply the learned PCA transformation to our data.\n",
        "\n",
        "The `PCA` class takes an optional argument `n_components`. If you know that you want to reduce your data to a fixed number of dimensions `d`, then you can set `n_components=d` and the `.transform` method will only return the first `d` dimensions. \n",
        "\n",
        "In our case, we aim to figure out what a good value for `d` could be, so we do not specify it. In this case the output of `PCA` will have the same number of dimensions as the input, i.e. it will compute all Principal Components. "
      ],
      "metadata": {
        "id": "QWIaJFtsUyb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(x_train_norm)\n",
        "\n",
        "x_train_pca = pca.transform(x_train_norm)\n",
        "x_test_pca = pca.transform(x_test_norm)\n",
        "\n",
        "print(\"Number of dimensions before: \", x_train_norm.shape[1])\n",
        "print(\"Number of dimensions after: \", x_train_pca.shape[1])"
      ],
      "metadata": {
        "id": "8N88QiKa6jNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In class you have seen that you can select a good number of principal components by looking at the *explained variance* of each component.\n",
        "\n",
        "The `sklearn` implementation of `PCA` stores this information in the fields `.explained_variance_` and `.explained_variance_ratio_`. The first contains the absolute variance and the second the percentage.\n",
        "\n",
        "Below, we plot the percentage of explained variance for each component in *blue* and the cumulative explained variance in *green*.\n",
        "\n",
        "### TASK: Study the resulting plot:\n",
        "* How many principal components should we keep?\n",
        "* Why?"
      ],
      "metadata": {
        "id": "SUdnJ8g1Zroq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explained_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "n_components = list(range(1, 31))  # [1, 2, ..., 30]\n",
        "plt.plot(n_components, explained_ratio, marker='o', color='blue')\n",
        "plt.plot(n_components, explained_ratio.cumsum(), marker='', color='green', drawstyle=\"steps-post\")"
      ],
      "metadata": {
        "id": "svufQ5i48t8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2b. Plotting Principal Components\n",
        "\n",
        "We can now plot different pairs of principal components. In the cell below you can change the values of `component_1` and `component_2` to see what happens.\n",
        "\n",
        "In particular, compare the plot for `component_1 = 0` and `component_2 = 1` to the one for `component_1 = 13` and `component_2 = 22`.\n",
        "\n",
        "What do you observe?"
      ],
      "metadata": {
        "id": "EuRhOExIdT3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "component_1 = 13  # enter here any number between 0..29\n",
        "component_2 = 22\n",
        "\n",
        "plt.scatter(x_train_pca[:, component_1], x_train_pca[:, component_2], c=y_train.map({'Benign': 0, 'Malignant': 1}), s=50)\n",
        "plt.xlabel(f\"Component {component_1}\")\n",
        "plt.ylabel(f\"Component {component_2}\")"
      ],
      "metadata": {
        "id": "jCEqZ-I5dS_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Enter the following in **Moodle**\n",
        "\n",
        "* Task 2a (2 Points): How many principal components should we keep and why?\n",
        "* Task 2b (1 Point): Your observation."
      ],
      "metadata": {
        "id": "hAt2nOVRbfcb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How well can we classify our data if we reduce the number of dimensions?\n",
        "\n",
        "In the cell below, we want to find out how many principal components we need to keep to get the same (or better) performance as our baseline from Task 1.\n",
        "\n",
        "Fill out the missing code below. Remember that we use `f1_malignant` to compare different classifiers. You should use a `LinearSVC` and you do **not** have to set any hyperparameter (`C` or `class_weight`).\n"
      ],
      "metadata": {
        "id": "ylOGxweSeWPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### TASK 2c. How many components are needed to get the same or better result than our baseline?"
      ],
      "metadata": {
        "id": "ix5SJX3nbVFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n_components in range(1, 31):\n",
        "  x_train_reduced = x_train_pca[:, :n_components]  # only use the first n components\n",
        "  x_test_reduced = x_test_pca[:, :n_components]    # only use the first n components\n",
        "\n",
        "  svm = LinearSVC()  # You do not need to change C and class_weight\n",
        "  \n",
        "  # fit svm and compute f1_malignant\n",
        "  # how many components result in the best score?\n"
      ],
      "metadata": {
        "id": "GvkRYhPCFhRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Enter in **Moodle** whether you solved this task (2c, 2 Points).\n"
      ],
      "metadata": {
        "id": "1v-7BSubhNHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: A bigger Dataset (2 Points)\n",
        "\n",
        "In this task, we will repeat the main points from Tasks 1 and 2 but for a dataset with a lot more features (100000).\n",
        "\n",
        "The goal is to classify chemical compounds into \"active\" and \"inactive\", meaning whether they will bind to a certain receptor.\n",
        "\n",
        "Below we load the dataset and we also include the dataset description from the openml.org website."
      ],
      "metadata": {
        "id": "Zkld2tAah1n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openml\n",
        "data = openml.datasets.get_dataset(dataset_id=4137)\n",
        "X, y, _, _ = data.get_data(target=\"class\")\n",
        "y = y.sparse.to_dense()\n",
        "y.replace(0, 'Inactive', inplace=True)\n",
        "y.replace(1, 'Active', inplace=True)\n",
        "\n",
        "from collections import Counter\n",
        "print(\"Number of features: \", X.shape[1])\n",
        "print(\"Class balance: \", Counter(y))"
      ],
      "metadata": {
        "id": "lHy5Bx9VqCZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description from [openml.org](https://https://www.openml.org/search?type=data&sort=runs&status=active&id=4137): \n",
        "\n",
        "DOROTHEA is a drug discovery dataset. Chemical compounds represented by\n",
        "structural molecular features must be classified as active \n",
        "(binding to thrombin) or inactive. \n",
        "This is one of 5 datasets of the NIPS 2003 feature selection challenge.\n",
        "\n",
        "Source:\n",
        "\n",
        "a. Original owners The dataset with which DOROTHEA was created is one of the KDD (Knowledge Discovery in Data Mining) Cup 2001. \n",
        "The original dataset and papers of the winners of the competition are available at: http://www.cs.wisc.edu/~dpage/kddcup2001/. \n",
        "DuPont Pharmaceuticals graciously provided this data set for the KDD Cup 2001 competition.\n",
        "All publications referring to analysis of this data set should acknowledge DuPont Pharmaceuticals Research Laboratories and KDD Cup 2001.\n",
        "\n",
        "b. Donor of database This version of the database was prepared for the NIPS 2003 variable and feature selection benchmark by Isabelle Guyon, 955 Creston Road, Berkeley, CA 94708, USA (isabelle '@' clopinet.com).\n",
        "\n",
        "Data Set Information:\n",
        "\n",
        "Drugs are typically small organic molecules that achieve their desired activity by binding to a target site on a receptor. The first step in the discovery of a new drug is usually to identify and isolate the receptor to which it should bind, followed by testing many small molecules for their ability to bind to the target site. This leaves researchers with the task of determining what separates the active (binding) compounds from the inactive (non-binding) ones. Such a determination can then be used in the design of new compounds that not only bind, but also have all the other properties required for a drug (solubility, oral absorption, lack of side effects, appropriate duration of action, toxicity, etc.). The original data were modified for the purpose of the feature selection challenge. In particular, we added a number of distractor feature called 'probes' having no predictive power. The order of the features and patterns were randomized.\n",
        "\n",
        "DOROTHEA -- Positive ex. -- Negative ex. -- Total Training set -- 78 -- 722 -- 800 Validation set -- 34 -- 316 -- 350 Test set -- 78 -- 722 -- 800 All -- 190 -- 1760 -- 1950\n",
        "\n",
        "We mapped Active compounds to the target value +1 (positive examples) and Inactive compounds to the target value â€“1 (negative examples).\n",
        "\n",
        "Number of variables/features/attributes: Real: 50000 Probes: 50000 Total: 100000"
      ],
      "metadata": {
        "id": "jiLAvCXPx-Kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train / Test Split"
      ],
      "metadata": {
        "id": "Si5QNHc0zu_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    train_size=.8,\n",
        "    shuffle=True,\n",
        "    random_state=RANDOM_SEED,\n",
        "    stratify=y,\n",
        ")"
      ],
      "metadata": {
        "id": "o2TwkJ1-jPcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Metric\n",
        "\n",
        "The goal of the dataset is to accurately determine whether a compound is \"active\". Therefore we define the F1 score for the \"Active\" class as our target measure to assess the preformance of a classifier."
      ],
      "metadata": {
        "id": "AL-A1Ctxz56c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_active(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
        "  \"\"\"\n",
        "  Helper function to compute the F1 score for the 'Active' class\n",
        "\n",
        "  y_true: a pd.Series object containing the ground truth labes \n",
        "          ('Inactive' or 'Active'), you will mainly pass 'y_test' here\n",
        "  y_pred: a pd.Series object containing predicted labels, for example the \n",
        "          output of the '.predict' method of a classifier\n",
        "  \"\"\"\n",
        "  return f1_score(\n",
        "      y_true=y_true,\n",
        "      y_pred=y_pred,\n",
        "      pos_label=\"Active\",\n",
        "      average=\"binary\",\n",
        "  )"
      ],
      "metadata": {
        "id": "Vo3t_w1rsvnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Classifier"
      ],
      "metadata": {
        "id": "n0ueVs5t0M7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_clf = LinearSVC(class_weight='balanced')  # use class_weight=\"balanced\" since our dataset has very high label inbalance\n",
        "baseline_clf.fit(x_train, y_train)\n",
        "baseline_predictions = baseline_clf.predict(x_test)\n",
        "\n",
        "f1_baseline = f1_active(y_test, baseline_predictions)\n",
        "\n",
        "print(\"Baseline F1 (Active): \", f1_baseline)"
      ],
      "metadata": {
        "id": "yeciw7sHv1uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Truncated SVD\n",
        "\n",
        "Our dataset is very sparse. This means that a majority of the entries in our feature matrix are 0. In such cases it is wasteful to explicitely store those 0 values and instead more efficient representations are used (a naive one would for example store the row and column indices and the associated value for each non-zero entry).\n",
        "\n",
        "Unfortunately, the `sklearn` implementation of `PCA` does not support sparse matrices and converting our data to a dense representation can lead to RAM issues. Therefore, we use a \"cousin\" of PCA named `TruncatedSVD` that can handle sparse data. The two algorithms work almost the same way and we will use it interchangeably here.\n",
        "\n",
        "**Warning:** this can take a few minutes to compute."
      ],
      "metadata": {
        "id": "IU6MN1Wt0ppZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "tsvd = TruncatedSVD(n_components=2000)\n",
        "tsvd.fit(x_train)\n",
        "\n",
        "x_train_pca = tsvd.transform(x_train)\n",
        "x_test_pca = tsvd.transform(x_test)"
      ],
      "metadata": {
        "id": "08HzT4KHp3mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Variances Explained for each Component\n",
        "\n",
        "Here we plot again the variance explained by each component and the cumulative explained variance. We limit ourselves to the first 200 components for this plot."
      ],
      "metadata": {
        "id": "A6ZKbTRR11Kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explained_ratio = tsvd.explained_variance_ratio_\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(np.arange(1, 201), explained_ratio[:200], marker='o', color='blue')\n",
        "plt.plot(np.arange(1, 201), explained_ratio.cumsum()[:200], marker='', color='green', drawstyle=\"steps-post\")"
      ],
      "metadata": {
        "id": "dRBRzkZfyZii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the number of components we should keep for classification?"
      ],
      "metadata": {
        "id": "YOWisY0C7r7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for n_components in [10, 25, 50, 100, 200]:\n",
        "  x_train_reduced = x_train_pca[:, :n_components]\n",
        "  x_test_reduced = x_test_pca[:, :n_components]\n",
        "  svm = LinearSVC(class_weight=\"balanced\", max_iter=100000)  # we increase the number of optimization steps to make sure the SVM converges\n",
        "  svm.fit(x_train_reduced, y_train)\n",
        "  f1 = f1_active(y_test, svm.predict(x_test_reduced))\n",
        "  print(n_components, f1)"
      ],
      "metadata": {
        "id": "C1NEKmQQ3N_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3a. Runtime Considerations\n",
        "\n",
        "You have just seen that by using dimensionality reduction, we can drastically improve the classification performance.\n",
        "\n",
        "Another consideration is the runtime behaviour. By reducing the number of dimensions used for training we can reduce the training time by an order of magnitude.\n",
        "\n",
        "An easy way to measure the time it takes to execute a statement, we can use the macro `%timeit` built into IPython shells and notebooks. The macro will execute the statement a number of times and report the average time to execute the statement."
      ],
      "metadata": {
        "id": "6ooiN1TSq1w7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = LinearSVC(class_weight=\"balanced\", max_iter=100000)"
      ],
      "metadata": {
        "id": "aurJYRXwdC6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we use `%timeit` to measure how long it takes to fit an SVM on all dimensions."
      ],
      "metadata": {
        "id": "XsPjbewZshaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%timeit svm.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "DJH-qtVzdHaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task: measure the time it takes to fit an SVM on dimensionality reduced data\n",
        "\n",
        "You can change `n_components` to change the number of dimensions used. You can reuse the `svm` variable instantiated above."
      ],
      "metadata": {
        "id": "2bqZKfn5sxKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_components = 20\n",
        "x_train_reduced = x_train_pca[:, :n_components]\n",
        "\n",
        "# use %timeit to measure how long it takes to fit the svm on x_train_reduced"
      ],
      "metadata": {
        "id": "ppkMgvNCdUwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Enter the following in **Moodle**\n",
        "\n",
        "* The average runtime for `n_components=20`\n",
        "* The average runtime for `n_components=200`"
      ],
      "metadata": {
        "id": "4YC0Sn4ht-HD"
      }
    }
  ]
}