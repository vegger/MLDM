{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegger/MLDM/blob/main/labs/L06_SVM_LAB_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAJRKdv9QA4C"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTVOKVltsG4d"
      },
      "source": [
        "# TASK 1. SVM (4 Points): Support Vector Machines\n",
        "\n",
        "**Support vector machines (SVMs)** are a particularly powerful and flexible class of supervised algorithms for both classification and regression. In this section, we will develop the intuition behind support vector machines and their use in **classification problems**. \n",
        "\n",
        "The goal is to find a line or curve (in two dimensions) or manifold (in higher dimensions) that **separates the classes from each other**. The SVM tries to separate the two sets with the **biggest margin possible**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4j15WKTKPn1b"
      },
      "source": [
        "Note: In order to plot the data and the decision boundaries of the applied model, we will use the function `plot_svc_decision_function` for multiple sub-tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1DKr5ItIWQG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Helper function to plot the decision boundary of an Support Vector Machine for Classification\n",
        "# You do not need to understand how it works.\n",
        "\n",
        "def plot_svc_decision_function(model, X, y):\n",
        "    \"\"\"\n",
        "    Plot the decision function for a 2D SVC\n",
        "\n",
        "    model: a fitted SVM model of type sklearn.svm.LinearSVC or sklearn.svm.SVC\n",
        "    X: a 2D numpy array of shape [n_samples, 2], containing data points to be classified\n",
        "    y: a 1D numpy array of shape [n_samples], containing the labels for the data points, labels should be 0 or 1\n",
        "    \"\"\"\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA0LWVdnQzlU"
      },
      "source": [
        "Let's create artificial data with two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbvdP62EIWQF",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFEsfwbBIWQG"
      },
      "source": [
        "### Applying SVM \n",
        "\n",
        "In this task we will use the `LinearSVC` model of `sklearn.svm`. `sklearn`'s module `svm` has implementations of several Support Vector Machine algorithms. Some are meant to solve Regression, some - Classification problems. `LinearSVC` stands for Linear Support Vector Classification which is exactly what we need in order to solve the present problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lkm6FC-1KsD",
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "model = LinearSVC() # initialize the model\n",
        "model.fit(X, y) # fit the model = learn the decision boundaries\n",
        "\n",
        "plot_svc_decision_function(model, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd8_ENRCi5ch"
      },
      "source": [
        "In the plot you see a solid and 2 dashed lines. \n",
        "\n",
        "The solid line shows the decision boundary, meaning points on one side will be assigned to the yellow class and on the other side to the red class. In this case all\n",
        "data points lie on the correct side of the decision boundary, meaning our classifier has successfully learned to separate the classes.\n",
        "\n",
        "The dashed lines visualize the margin of the SVM classifier. You can see that a\n",
        "few points lie inside the margin. This is because, in practice, we use a soft-margin implementation for SVM. This means that we allow data points to be inside the margin, or even on the wrong side of the boundary, but during training we \"punish\" these points.\n",
        "\n",
        "In the next excercise you will play with the `C` parameter of `LinearSVC`. This\n",
        "parameter controls how much we \"punish\" points inside (or on the wrong side) of the margin.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0JJcOdK1KsE"
      },
      "source": [
        "### 1a. Regularization\n",
        "Try different values for the parameter `C` and try to find out what happens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sn2zmKrr1KsE"
      },
      "outputs": [],
      "source": [
        "for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    # create an instance of LinearSVC and set its parameter C\n",
        "    # call its .fit method to train it on X and y\n",
        "    \n",
        "    model = ...  \n",
        "\n",
        "    plot_svc_decision_function(model, X, y)\n",
        "    plt.title(f\"C={c}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l9Ebxgp1KsE"
      },
      "source": [
        "### 1b. We add a new datapoint\n",
        "\n",
        "We will now add a new red data point, that lies very close to the cluster of yellow points.\n",
        "\n",
        "In real world datasets, we run into this situation all the time. One reason can be that some data points really do look very similar but belong to different classes (e.g. imagine distinguishing a crocodile from an alligator). Another reason could be that the data point would really belong to the other (yellow) class but has been annotated badly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otatVaHS1KsF"
      },
      "outputs": [],
      "source": [
        "X2 = np.append(X, [[1., 1.5]], axis=0)\n",
        "y2 = np.append(y, [0], axis=0)\n",
        "\n",
        "plt.scatter(X2[:, 0], X2[:, 1], c=y2, s=50, cmap='autumn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Awtugu1h1KsF"
      },
      "source": [
        "Rerun the examle from above, i.e. train a `LinearSVC` with different values of `C` and see what changes.\n",
        "\n",
        "Pay particular attention to how the new data point changes the outcome."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Nod5tv1KsF"
      },
      "outputs": [],
      "source": [
        "for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    # create an instance of LinearSVC and set its parameter C\n",
        "    # call its .fit method to train it on X2 and y2\n",
        "    \n",
        "    model = ...  \n",
        "\n",
        "    plot_svc_decision_function(model, X2, y2)\n",
        "    plt.title(f\"C={c}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON8ogrmw1q4k"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Answer following question in Moodle:\n",
        "\n",
        "Based on the experiments above, which value for `C` would you choose for the new data, and **why**?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz4yA62HAyB6"
      },
      "source": [
        "# TASK 2. Support Vector Machines - Unbalanced Data (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4BAX9lf1KsG"
      },
      "source": [
        "How to handle unbalanced classes with SVM? Below you see an example of an unbalanced problem. There are 1000 red samples and just 100 yellow samples. The SVM is very sensitive to this. Here we will explore the SVMs behaviour in the unbalanced case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuh0m5zy1KsG",
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10.0, 10.0))\n",
        "\n",
        "# we create clusters with 1000 and 100 points\n",
        "rng = np.random.RandomState(0)\n",
        "n_samples_1 = 1000\n",
        "n_samples_2 = 100\n",
        "X_unbal = np.r_[1.5 * rng.randn(n_samples_1, 2), 0.5 * rng.randn(n_samples_2, 2) + [3, 1]]\n",
        "y_unbal = [0] * (n_samples_1) + [1] * (n_samples_2)\n",
        "\n",
        "plt.scatter(X_unbal[:, 0], X_unbal[:, 1], c=y_unbal, cmap='autumn', edgecolors='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV76eoPA1KsG"
      },
      "source": [
        "### Default solution\n",
        "\n",
        "In the cells below, you see a simple way to classify the above data.\n",
        "* We use `LinearSVC` with default parameters as our model and fit it on the unbalanced data\n",
        "* We visualize the resulting decision boundary\n",
        "* We compute performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcSVZZPopzFP"
      },
      "outputs": [],
      "source": [
        "# This should look familiar by now.\n",
        "\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_unbal, y_unbal)\n",
        "\n",
        "plot_svc_decision_function(clf, X_unbal, y_unbal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QZiBkt-qHLF"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# get predictions from the classifier\n",
        "# it will assign red to all points to the left of the solid line and yellow to the points on the right side  \n",
        "y_pred = clf.predict(X_unbal)\n",
        "\n",
        "# the function classification_report computes precision, recall, and F1 score\n",
        "# and returns a nicely formatted string\n",
        "# it takes the true labels as argument y_true and predicted labels as y_pred\n",
        "# the digits argument controls how many decimal points are printed\n",
        "# the support column in the output shows how many data points belong to each class\n",
        "print(classification_report(y_true=y_unbal, y_pred=y_pred, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmJlEUOTwx0G"
      },
      "source": [
        "### 2a. Handle unbalanced data using class weights\n",
        "\n",
        "In this exercise we will explore how the `class_weight` parameter can be used to handle unbalanced data.\n",
        "\n",
        "We have seen previously that `C` \"punishes\" data points that lie on the wrong side of the classification boundary. The `class_weight` parameter follows a similar idea. It allows us to control how harshly we punish points on the wrong\n",
        "side for each class separately. For example, we could choose to force more yellow points to be on the right of the boundary, by giving them more *weight*. This means they will be counted more during training and mis-classifications are punished more.\n",
        "\n",
        "The default value for the `class_weight` parameter is `None` and means both\n",
        "classes will have a weight of 1.\n",
        "\n",
        "You can manually set weights using a dictionary:\n",
        "```\n",
        "my_class_weights = {\n",
        "  0: 1.0,\n",
        "  1: 1.0,\n",
        "}\n",
        "```\n",
        "The keys correspond to the class labels (in our case 0 for red and 1 for yellow)\n",
        "and the values correspond to the weight we want to set. We can then set `class_weight=my_class_weights`.\n",
        "\n",
        "\n",
        "Finally, you can specify `class_weight='balanced'`. In that case, `sklearn` tries to automatically determine good class weights. Let $n$ be the total number of samples (1100 in our case), $c$ the total number of classes (2 in our case), and $n_i$ the number of samples in class $i$ (in our case $n_0 = 1000$ and $n_1 = 100$, then it computes the class weights $w_i$ as $w_i = \\frac{n}{c n_i}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0YF9vUtJA02"
      },
      "source": [
        "Choose a proper parameter for `class_weight` for example by manually trying out different values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3IhX4Xh1KsI",
        "jupyter": {
          "outputs_hidden": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# fit the model using weighted classes\n",
        "\n",
        "# change the class weights\n",
        "my_class_weights = {\n",
        "    0: 1.0,  # weight for red class\n",
        "    1: 1.0,  # weight for yellow class\n",
        "}\n",
        "wclf = ...  # create a LinearSVC and set its class_weight parameter\n",
        "wclf.fit(X_unbal, y_unbal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfNefU3d1KsI"
      },
      "outputs": [],
      "source": [
        "plot_svc_decision_function(wclf, X_unbal, y_unbal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5ur7dOkxdaM"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=y_unbal, y_pred=wclf.predict(X_unbal), digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3w3Hop_46Af"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Post following results in Moodle:\n",
        "\n",
        "1. The code how you called the `LinearSVC` with proper `class_weight` settings. From the task 2a.\n",
        "2. The classification report from the task 2a.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBJKSAuw79su"
      },
      "source": [
        "# Task 3. SVM Kernel Trick (3 points)\n",
        "In this task we will investigate the case of non-linearly separable data. In order to handle such a case, the **Kernel Trick** can be used. We transform our data and map it into a **higher dimensional feature space** (e.g., if the data had two features (2D-space), it becomes 3D-space). The goal is that after the transformation to the higher dimensional space, the classes will be linearly separable. The decision boundary can then be fitted to separate the classes and make predictions. The decision boundary will be a hyperplane in this higher dimensional space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1be1nzl1KsJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X_non_linear, y_non_linear = make_circles(100, factor=.1, noise=.1)\n",
        "plt.scatter(X_non_linear[:, 0], X_non_linear[:, 1], c=y_non_linear, cmap='autumn', edgecolors='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBDHhAPb1KsJ"
      },
      "source": [
        "### Default model\n",
        "\n",
        "Below you can see what happens if we naively train a `LinearSVC` model on this non-linear dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI_lTRAu1KsJ"
      },
      "outputs": [],
      "source": [
        "clf = LinearSVC().fit(X_non_linear, y_non_linear)\n",
        "plot_svc_decision_function(clf, X_non_linear, y_non_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAYJ2Qxf8CFi"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=y_non_linear, y_pred=clf.predict(X_non_linear), digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3RtWVcn1KsJ"
      },
      "source": [
        "### Manually Adding a Helper Dimension\n",
        "\n",
        "It is clear that no linear discrimination will ever be able to separate this data. We can think about how we might project the data into a higher dimension such that a linear separator would be sufficient. In the code below we compute a new value `r` based on the data points. Adding `r` as a new dimension to our data, we will see that the data becomes linearly separable.\n",
        "\n",
        "We compute $r = e^{-||x||^2}$. We chose this because the data points lie on circles and $||x||^{2}$ corresponds to the radius of the circle that a data point lies on.\n",
        "\n",
        "**Note** that this new dimension `r` is ONLY for the visualization of the data. In the next subtask, we will use the RBF kernel on the unchanged data to let the SVM find a proper projection by itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ_cZCwx1KsJ"
      },
      "outputs": [],
      "source": [
        "r = np.exp(-(X_non_linear ** 2).sum(1))\n",
        "\n",
        "fig1=plt.figure(figsize=(10, 10))\n",
        "ax = fig1.add_subplot(projection='3d')\n",
        "ax.scatter3D(X_non_linear[:, 0], X_non_linear[:, 1], r, c=y_non_linear, s=50, cmap='autumn')\n",
        "ax.view_init(elev=30, azim=45)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V54dBD161KsK"
      },
      "source": [
        "### 3a. Kernel Trick\n",
        "\n",
        "To use the kernel trick, we need to replace `LinearSVC` by `SVC` and set the `kernel` we want to use. \n",
        "\n",
        "Use `kernel='rbf'` and fit an `SVC` model on `X_non_linear` and `y_non_linear`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VfmCiV71KsK"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# create an SVC an set its kernel parameter to 'rbf'\n",
        "# train / fit it on X_non_linear and y_non_linear\n",
        "clf = ...  \n",
        "\n",
        "plot_svc_decision_function(clf, X_non_linear, y_non_linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCcdm8I38XKr"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true=y_non_linear, y_pred=clf.predict(X_non_linear), digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JkHrkdl97W-"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Please answer in Moodle whether you solved this task.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}