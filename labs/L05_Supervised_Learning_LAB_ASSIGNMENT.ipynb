{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vegger/MLDM/blob/main/labs/L05_Supervised_Learning_LAB_ASSIGNMENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAJRKdv9QA4C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1m4qcrpXdTt"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 0x0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia9s_Q-KXf0T"
      },
      "source": [
        "# TASK 1. Metrics (4 Points): \n",
        "In this task you will compute some standard quality measures like Precision, Recall and F-Score for an artificial dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV0Z3OdeXpha"
      },
      "source": [
        "First, we generate some artificial data for a classification task and take a look at it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ_TBTXQfq_Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X, y = make_blobs(n_samples=500, centers=4, random_state=RANDOM_SEED, cluster_std=2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51zAL1iNwwpb"
      },
      "outputs": [],
      "source": [
        "for target_class in np.unique(y):\n",
        "  plt.scatter(X_train[y_train==target_class, 0], X_train[y_train==target_class, 1], alpha=0.75, label=target_class)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyZqEqjVyDoO"
      },
      "source": [
        "As we can see, the classes are not easy distinguishable. That is not an easy task for the model. \n",
        "Nevertheless, let's apply a Logistic Regression Model and predict `y` values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0izCFFL0WaP"
      },
      "outputs": [],
      "source": [
        "log_reg = LogisticRegression(random_state=RANDOM_SEED).fit(X_train, y_train)\n",
        "y_test_pred = log_reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Mb3vlL3iyX"
      },
      "source": [
        "### Task 1a. Create and visualise the confusion matrix that describes the results:\n",
        "\n",
        "1. Create confusion matrix. Use can use `sklearn.metrics.confusion_matrix` functions.\n",
        "2. Display the confusion matrix of the Logistic Regression Model with `seaborn.heatmap`. Include the numbers of samples in each cell of the heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCZgTYP3fZe7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2-ZAlHQQOG2"
      },
      "source": [
        "### Task 1b. Create functions that calculate Precision, Recall and F1-Score. \n",
        "1. Implement your own functions for calculating Precision, Recall and F1-Score. Don't use any of the existing libraries for this.\n",
        "2. Apply them on the example above for the class `0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9X3Q8Je3B3B"
      },
      "outputs": [],
      "source": [
        "def precision(class_of_interest, confusion_matrix_test):\n",
        "  ...\n",
        "  return score\n",
        "\n",
        "def recall(class_of_interest, confusion_matrix_test):\n",
        "  ...\n",
        "  return score\n",
        "\n",
        "def f1_score(precision_value, recall_value):\n",
        "  ...\n",
        "  return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghV4-8vB7c6b"
      },
      "outputs": [],
      "source": [
        "precision_value = precision(...)\n",
        "recall_value = recall(...)\n",
        "f1_score_test = f1_score(...)\n",
        "\n",
        "print(f\"Precision: {precision_value}\")\n",
        "print(f\"Recall: {recall_value}\")\n",
        "print(f\"F1-Score: {f1_score_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34pKmkc1COk"
      },
      "source": [
        "### Task 1c. Check your results \n",
        "Use the function `classification_report` of `sklearn.metrics` to compare their results to your own implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZraefM_U8wdN"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiCZPpG1-X1n"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Post following results you got in this task:\n",
        "\n",
        "1. Confusion matrix from task 1a\n",
        "2. Precision, Recall and F1-Score from task 1b\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz4yA62HAyB6"
      },
      "source": [
        "# TASK 2. Cross Validation (1 point)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prescribed-lawyer"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randrange\n",
        "import seaborn as sns\n",
        "sns.set() # just the theme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd1qwbP1QZdm"
      },
      "source": [
        "In this task, we familiarize ourselves with how different types of Cross Validation actually partition the data. \n",
        "\n",
        "Run the following code and inspect the graphs that show how the data was split for different runs. Full code source can be found [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HWH1kVh36sv"
      },
      "source": [
        "Note: the bar \"class\" on the graphs shows the samples of 3 classes. First, all samples of class 0 are listed (in light blue color), then all samples of class 1 are shown (yellow) and, lastly, all samples of class 2 are following (brown)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KYeoYGskek3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import (\n",
        "    KFold,\n",
        "    ShuffleSplit,\n",
        "    StratifiedKFold,\n",
        "    StratifiedShuffleSplit\n",
        ")\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# first, generate dummy data\n",
        "n_points = 100\n",
        "rng = np.random.RandomState(RANDOM_SEED)\n",
        "X = rng.randn(100, 10)\n",
        "\n",
        "# we have 10% of the data belonging to class 0, 30% - to class 1 and 60% to class 2\n",
        "# first come all the data samples that belong to class 0, following by the samples of class 1 and then of class 2\n",
        "percentiles_classes = [0.1, 0.3, 0.6]\n",
        "y = np.hstack([[ind] * int(100 * perc) for ind, perc in enumerate(percentiles_classes)]) \n",
        "\n",
        "# color settings for graphs\n",
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm\n",
        "\n",
        "n_splits = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDyXvwEfn2gU"
      },
      "outputs": [],
      "source": [
        "cvs = [KFold, ShuffleSplit, StratifiedKFold, StratifiedShuffleSplit]\n",
        "\n",
        "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10):\n",
        "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
        "\n",
        "    # Generate the training/testing visualizations for each CV split\n",
        "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
        "      \n",
        "        # Fill in indices with the training/test groups\n",
        "        indices = np.array([np.nan] * len(X))\n",
        "        indices[tt] = 1.0\n",
        "        indices[tr] = 0\n",
        "\n",
        "        # Visualize the results\n",
        "        ax.scatter(\n",
        "            range(len(indices)),\n",
        "            [ii + 0.5] * len(indices),\n",
        "            c=indices,\n",
        "            marker=\"_\",\n",
        "            lw=lw,\n",
        "            cmap=cmap_cv,\n",
        "            vmin=-0.2,\n",
        "            vmax=1.2\n",
        "        )\n",
        "\n",
        "    # Plot the data classes and groups at the end\n",
        "    ax.scatter(\n",
        "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n",
        "    )\n",
        "    # Formatting\n",
        "    yticklabels = list(range(n_splits)) + [\"class\"]\n",
        "    ax.set(\n",
        "        yticks=np.arange(n_splits + 1) + 0.5,\n",
        "        yticklabels=yticklabels,\n",
        "        xlabel=\"Sample index\",\n",
        "        ylabel=\"CV iteration\",\n",
        "        ylim=[n_splits + 1.2, -0.2],\n",
        "        xlim=[0, 100],\n",
        "    )\n",
        "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
        "    return ax\n",
        "\n",
        "for cv in cvs:\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    plot_cv_indices(cv(n_splits), X, y, ax, n_splits)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3w3Hop_46Af"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Answer following question in Moodle:\n",
        "\n",
        "Try to explain in your own words (1-2 lines) how ShuffleSplit splits the data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OM0Huw-Cd2D"
      },
      "source": [
        "# Task 3. Cross Validation and Imbalanced Data (5 points)\n",
        "In this task we are working with the **Default of Credit Card Clients Dataset**. This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. More info about the dataset can be found [here](https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset).\n",
        "\n",
        "The target variable is:\n",
        "\n",
        "- `default.payment.next.month`: Default payment (1=yes, 0=no)\n",
        "\n",
        "Default payment means a missed payment. So, the target variable shows whether a person will miss his or her Credit Card Payment (=1) or will pay it back (=0).\n",
        "\n",
        "The predictor features are:\n",
        "*   ID: ID of each client\n",
        "*   LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
        "*   SEX:0, SEX:1: Binary variables for gender (0=male, 1=female)\n",
        "*   EDUCATION:0-EDUCATION:5: Binary variables for education (0=graduate school, 1=university, 2=high school, 3=others, 4=unknown, 5=unknown)\n",
        "*   MARRIAGE:0-MARRIAGE:2: Binary variables for Marital status (0=married, 1=single, 2=others)\n",
        "*   AGE: Age in years\n",
        "*   PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "*   PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "*   PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "*   PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "*   PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "*   PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "*   BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "*   BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "*   BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "*   BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "*   BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "*   BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "*   PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "*   PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "*   PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "*   PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "*   PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "*   PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS9Ep-RnXTKb"
      },
      "outputs": [],
      "source": [
        "# install datasets\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOXZGVmC-tMj"
      },
      "source": [
        "First, we load and inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUC4q-M6W32q"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imodels/credit-card\")\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "X_train = df_train.drop(columns=['default.payment.next.month'])\n",
        "y_train = df_train['default.payment.next.month'].values\n",
        "print(X_train.head())\n",
        "\n",
        "df_test = pd.DataFrame(dataset['test'])\n",
        "X_test = df_test.drop(columns=['default.payment.next.month'])\n",
        "y_test = df_test['default.payment.next.month'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Swjjt7MO4_gO"
      },
      "source": [
        "In the following, we want to explore how to handle highly imbalanced data. \"Unfortunately\", the dataset is currently not very imbalenced. For this reason we implement an auxiliary function that deletes X% of all samples of class 1. Applying this function several times will make our data more and more imbalanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOU7HR-mnJ6C"
      },
      "outputs": [],
      "source": [
        "def delete_random_samples_of_class_1(samples_X, samples_y, percentage=90):\n",
        "  # find samples of class 1\n",
        "  y1_indices = np.argwhere(samples_y==1)\n",
        "  y1_indices = y1_indices.reshape((y1_indices.shape[0],))\n",
        "  sampled_indices = np.random.choice(y1_indices, int(percentage/100*len(y1_indices)), replace=False)\n",
        "  # delete randomly chosen samples of class 1\n",
        "  new_samples_X = np.delete(samples_X.values, sampled_indices, axis=0)\n",
        "  new_samples_y = np.delete(samples_y, sampled_indices)\n",
        "  return new_samples_X, new_samples_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XscOKKU85Nhv"
      },
      "source": [
        "### 3a. Inspect target variable and apply function that increases the imbalance of the dataset\n",
        "1. Check how many samples of each class are present in the train and test set\n",
        "2. Apply `delete_random_samples_of_class_1`\n",
        "3. Check again how many samples of each class are present in the train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5Q04V4arsP8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9VTR8O-sgdQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdm_IEBfUQQq"
      },
      "source": [
        "### 3b. Standardization the data\n",
        "Apply Standardization to the new data. You could either use the functions you created for the previous labs or `StandardScaler` from the `sklearn` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk6hgbTRccDe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjnexaD2UyDJ"
      },
      "source": [
        "### 3c. Logistic Regression and Cross Validation\n",
        "Wer now apply Logistic Regression and one of the Cross Validation techniques from the example above to the data in order to predict `default.payment.next.month`. Proceed as follows:\n",
        "1. Set `n_splits` to the number of splits that results in 20% of validation data in each split, set maximum iterations to `500` and random state to `RANDOM_SEED`.\n",
        "2. Create and train the model\n",
        "3. Apply the model in order to predict target variable of the test set. \n",
        "4. Calculate Accuracy, Precision (macro) and Recall (macro) for the predicted values.\n",
        "5. Generate and visualize the confusion matrix of the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKkHoJqdx_wb"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "def logistic_regression_CV(n_splits, class_weight=None):\n",
        "  ...\n",
        "  return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW2h6OI5-XMv"
      },
      "outputs": [],
      "source": [
        "def return_statistics(y_true, y_pred):\n",
        "  accuracy = ...\n",
        "  precision = ...\n",
        "  recall = ...\n",
        "\n",
        "  print(f\"Accuracy: {accuracy}\")\n",
        "  print(f\"Precision (macro): {precision}\")\n",
        "  print(f\"Recall (macro): {recall}\")\n",
        "\n",
        "  #visualise confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWwImBQv-2gI"
      },
      "outputs": [],
      "source": [
        "y_pred = logistic_regression_CV(...)\n",
        "return_statistics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYKJVr-lKtp"
      },
      "source": [
        "### 3d. Handling imbalanced dataset\n",
        "1. Run the function `logistic_regression_CV` one more time. But this time add parameter `class_weight='balanced'` to the Logistic Regression Model. Adjust the function accordingly. \n",
        "2. Visualize the confusion matrix of the test set again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQhTBuaY_K4d"
      },
      "outputs": [],
      "source": [
        "y_pred = ...  # train logistic regression with class_weight='balanced', use cross-validation as in the 3c\n",
        "return_statistics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JkHrkdl97W-"
      },
      "source": [
        "## ðŸ“¢ **HAND-IN** ðŸ“¢: Please hand in the following:\n",
        "\n",
        "- The confusion matrices of 3c and 3d\n",
        "- How do you interpret the confusion matrix of 3c?\n",
        "- What is the major change that happend in 3d?\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}